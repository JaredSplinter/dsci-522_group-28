---
title: "Hotel Cancellation Predict Report"
bibliography: hotels_refs.bib
author: "Jared Splinter"
date: "11/27/2020"
always_allow_html: true
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(tidyverse)
```

# Predicting Hotel Booking Cancellation from Real World Hotel Bookings

Jared Splinter,

Created: 11/27/2020

# Summary


Here we attempt to build a classification model to predict whether a given hotel booking is likely to be canceled. A model was selected by comparing many classification algorithims and selecting the best one as the Random Forest classification algorithm. From there, hyperparameter optimization was performed and the best resulting model was selected.  Our final model was scored using f1 metric on the test data and received a 0.835 compared to the train data f1 score of 0.983. The model incorrectly predicted 11.4% of the test bookings and suggests that the final model has been overfit. The model as it is may help hotels with their revenue management however we recommend continued improvement of the model and further feature examination/engineering to reduce overfitting and improve test score.


# Introduction

The hospitality industry and hotels in particular suffer huge revenue losses due to booking cancellations and no shows. The revenue lost becomes a sunk cost when there is not enough time to book the room again before the date of stay [@xie2007service]. Hotels would like to get an estimate if a booking is likely to be cancelled as predicting cancellations is useful for a hotel's revenue management.

Here we ask if we can use a machine learning algorithm to predict whether a given hotel booking is likely to be canceled. Finding the conditions on which a booking is likely to be canceled can help a hotel improve the conditions and limit the number of cancellations they receive, thereby increasing their revenue. If a booking is likely to be canceled a hotel may also wish to implement higher cancellation fees to make up some of the lost revenue [@chen2011search]. If a machine learning algothrithm can accurately predict if a hotel booking will be canceled it could help hotels make up some of their lost revenue and potentially find ways in which to improve customer satisfaction.

# Methods

## Data

The data set used in this project comes from the Hotel Booking demand datasets from Antonio, Almeida and Nunes at Instituto Universit√°rio de Lisboa (ISCTE-IUL), Lisbon, Portugal [@antonio2019hotel]. The data was sourced directly from the Github Repository [here](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-02-11). The dataset contains real world data obtained from two hotels; one resort hotel and one city hotel. Each row represents an individual hotel booking due to arrive between July 1st, 2015 and August 31st, 2017. There are 31 columns describing 40,060 observations from the resort hotel and 79,330 observations from the city hotel totaling 119,390 bookings.

## Analysis

Many classification model algorithms were compared using cross-validation so the best classification model could be selected. 5 fold cross validation was selected as the data set is quite large and it was scored on f1, precision, recall and accuracy as there is class imbalance within the dataset. f1 scores are reported as it is a good balance between recall and precision scores. The classification models compared are: Dummy Classifier, Decision Tree, K-nearest neighbor, SVC with RBF kernel, Logistic Regression, and Random Forest. 

From there, Random Forest was chosen as the classification model and hyperparameter optimization was carried out using Random Search Cross-Validation. The hyperparameters optimized from Random Forest were `n_estimators` and `min_sample_split`. The best model from the Random Search Cross-validation was selected and used to fit on the train data and then to score on the test data. 4 features were dropped from the analysis: company, agent, reservation_status, and reservation_status_date.

The Python programming language [@Python] and the following Python packages were used to perform the analysis: docopt [@docoptpython], pandas [@pandaspaper; @pandas], sklearn [@scikit-learn], altair [@altair], numpy [@numpy]. The code used to perform the analysis and create this report can be found here: <https://github.com/UBC-MDS/dsci-522_group-28>

This report was created using the R programming language [@R] and the following R packages: knitr [@knitr], KableExtra [@Kable], tidyverse [@tidyverse].

# Results & Discussion

### Exploratory Data Analysis

In our investigation of the dataset we sought to understand which features might be useful for prediction. Reading about the data collected we immediately decided that the columns `reservation_status` and `reservation_status_date` should be omitted from the model as they contain information after the prediction target and thus would not be useful. After establishing that there was a class imbalance between we checked to see if the dataset was "complete" (ie. if the dataset was missing values). The results of the missing values are presented in Table 1.


```{r, echo=FALSE, warning= FALSE, message=FALSE}

missing_nums_df <- read_csv('../results/missing_summary.csv')

kable(missing_nums_df,
      caption = "Table 1. Predictors with missing values, number of values missing and percentage of values missing") %>%
  kable_styling(full_width = FALSE) 
```


As 94.37% of the values from `company` are missing we decided to also exclude this from the model. Finally, we also decided to omit `agent` from the model as we determined there was not enough information about the predictor. As we see in Table 1, almost 14% of training data does not have a value for `agent`. The predictor is assigned a numeric value but we are not sure how the id's are assigned or specific per hotel. We also observed that 115 out of 324 `agent` ids have less than 10 observations and 247 out of 324 `agent` ids have less than 100 observations. We also observe that 2 ids have over 10,000 observations corresponding to specific hotels suggesting a correlation however, we did not think for these reasons `agent` would be a good predictor.


Having chosen our predictors, we then plotted the distributions of the numeric features and separated classes by colour (blue for canceled, orange for not canceled). Many of the distributions are right skewed as they are dominated by 0 values. This may mean many of these numeric features may not be good predictors of the targets. A few numeric features that looked promising for prediction are `total_of_special_requests`, `required_car_parking_spaces`, `stay_in_week_nights` and `stay_in_weekend_nights` as they have wider distributions. The results for the numeric feature distributions are presented in Figure 1.

```{r , echo=FALSE, warning= FALSE, message=FALSE, fig.cap="Figure 1. Comparison of the numeric distributions of training data predictors between canceled and not canceled bookings.", out.width = '100%'}

knitr::include_graphics("../results/numeric_vs_target.svg")

```

We then decided to look at the categorical features of the dataset to visualize the differences between classes. To do this, we plotted a 2D heatmap of every categorical variable counting the number of observations for each. A categorical feature with visible differences in the heatmap between canceled and not canceled could be good predictors for the model. We find that in particular, `hotel`, `market_segment`, `reserved_room_type` and `customer_type` could be viable useful predictors. The results for the categorical heatmaps are presented in Figure 2.


```{r , echo=FALSE, warning= FALSE, message=FALSE, fig.cap="Figure 2. Comparison of the categorical features of training data predictors between canceled and not canceled bookings.", out.width = '100%'}

knitr::include_graphics("../results/cat_vs_target.svg")

```


### Model Results

We compared a few classification model algorithms using a 5 fold cross-validation. Models were scored on the f1 metric. The results of the cross-validation scores are presented in Table 2. Compared to the baseline Dummy Classifier all models scored much higher. Random Forest scored the highest validation f1 score followed by Decision Tree. However, the fit time of Random Forest was much longer than that of Decision Tree.

```{r, echo=FALSE, warning= FALSE, message=FALSE}

model_cv_df <- read_csv('../results/five_fold_cross_validation_result.csv')

kable(model_cv_df %>%  select(classifier_name, fit_time, score_time, validation_f1, train_f1), 
      caption = "Table 2. 5 fold Cross validation scores of classifier models") %>%
  kable_styling(full_width = FALSE) 
```

As Random Forest classifier scored the highest f1 validation score we decided to use it as our classification model for the dataset. The next step we took was to run hyperparameter optimization on the model. The hyperparameters optimized from Random Forest were `n_estimators` and `min_sample_split`. The best model from hyperparameter optimization had hyperparameters of `n_estimators = 700` and `min_sample_split = 4` as seen in Table 3. Table 3 only includes the top 4 results of the hyperparameter optimization to show comparision with other searches. The differences in mean test score is very little between the top 3 results indicating the hyperparameters may not change the models drastically.

```{r, echo=FALSE, warning= FALSE, message=FALSE}

hyperparameter_df <- read_csv('../results/random_forest_tuning_result.csv')

hyperparameter_df<- hyperparameter_df %>%  
                        arrange(rank_test_score) %>% 
                        rename(n_estimators = param_randomforestclassifier__n_estimators,
                               min_samples_split = param_randomforestclassifier__min_samples_split) %>% 
                        filter(rank_test_score < 5) %>% 
                        select(rank_test_score,n_estimators,min_samples_split, mean_test_score, mean_train_score)

kable(hyperparameter_df, 
      caption = "Table 3. Results of Hyperparameter Optimization for Random Forest Classifer") %>%
  kable_styling(full_width = FALSE) 
```

Having chosen our hyperparameters, we fit our final model to the train data and score it on the test data. The model scores an f1 score of 0.835 on the test data and a 0.983 f1 score on the training data. While, the results for the test data scores high, because the score for the training data is much higher, we acknowledge the model in its current state has been overfit to the training data. Overfitting can create problems for model deployment so the model will need to be reassessed before it can be released for use among hotels. About 11.4% of the test data has been incorrectly predicted and we are hopeful our model could be improved to decrease this number.


```{r , echo=FALSE, warning= FALSE, message=FALSE, fig.cap="Figure 3. Confusion matrix of model performance on test data.", out.width = '50%'}

knitr::include_graphics("../results/random_forest_confusion_matrix_test_data.png")

```

To further show the overfitting of the model, we compared the confusion matrix from the test data (Figure 3) and the confusion matrix from the train data (Figure 4). The confusion matrix of the train data scores much better than that of the confusion matrix of the test data. We would ideally hope for these ratios to be similar.

```{r , echo=FALSE, warning= FALSE, message=FALSE, fig.cap="Figure 4. Confusion matrix of model performance on train data.", out.width = '50%'}

knitr::include_graphics("../results/random_forest_confusion_matrix_train_data.png")

```

Some limitations from this model could come from the dataset, there is only data contained from 2 hotels which could limit the scope of bookings from other hotels. Another limitation could be from using the classifier model Random Forest, while it had a good metric score it does come with high variance, another classification model may perform better on the test data and have the additional benefit of reduced fit time. Furthermore, one final limitation of the model can be due to human behaviour, unpredictable occurrences could happen to anyone and cause circumstances to change, quantifying this as an expected error could e beneficial to indicate whether our model is within a good prediction range.

To improve this model, we propose further hyperparameter tuning with a wider range of numbers. This requires more time as running hyperparameter tuning with Random Forest is computationally expensive. Additionally, we suggest incorporating some feature engineering into the model to address if more features should be dropped. While understanding which features are important for hotels to reduce the number of cancellations, some of the predictors may not be useful and may be causing the model to learn unimportant factors.Furthermore, the issue of overfitting as mentioned above must be addressed so as not to have a model that has learned specific patterns. However, with our current model as there is not much drawback for a hotel in a wrong booking prediction, they may wish to use this model to get an understanding of a booking. There should be note, as mentioned above, that this model does not have a perfect prediction and predicted outcomes should not be solely relyed upon.


# References
